"""Repository for model retraining jobs and drift metrics using SQLAlchemy."""

import uuid
from collections.abc import Sequence
from datetime import UTC, datetime, timedelta
from decimal import Decimal
from typing import TYPE_CHECKING, Any

from sqlalchemy import select, text
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker

from gal_friday.dal.base import BaseRepository
from gal_friday.dal.models.drift_detection_event import DriftDetectionEvent
from gal_friday.dal.models.retraining_job import RetrainingJob

# RetrainingJob Pydantic model from pipeline might be used by service layer, not repo directly.

if TYPE_CHECKING:
    from gal_friday.logger_service import LoggerService


class RetrainingRepository(BaseRepository[RetrainingJob]):
    """Repository for RetrainingJob data persistence using SQLAlchemy."""

    def __init__(
        self, session_maker: async_sessionmaker[AsyncSession], logger: "LoggerService",
    ) -> None:
        """Initialize the retraining repository.

        Args:
            session_maker: SQLAlchemy async_sessionmaker for creating sessions.
            logger: Logger service instance.
        """
        super().__init__(session_maker, RetrainingJob, logger)

    async def save_job(self, job_data: dict[str, Any]) -> RetrainingJob:
        """Saves a retraining job.
        `job_data` should contain fields for RetrainingJob model.
        job_id should be a UUID. model_id and new_model_id (if present) should be UUIDs.
        """
        for key in ["job_id", "model_id", "new_model_id"]:
            if key in job_data and isinstance(job_data[key], str):
                job_data[key] = uuid.UUID(job_data[key])

        for key in ["start_time", "end_time", "created_at", "updated_at"]:
            if key in job_data and isinstance(job_data[key], str):
                 dt_obj = datetime.fromisoformat(job_data[key])
                 job_data[key] = dt_obj.replace(tzinfo=UTC) if dt_obj.tzinfo is None else dt_obj
            elif key in job_data and isinstance(job_data[key], datetime) and job_data[key].tzinfo is None:
                 job_data[key] = job_data[key].replace(tzinfo=UTC)


        # Ensure job_id is present
        if "job_id" not in job_data:
            job_data["job_id"] = uuid.uuid4()

        return await self.create(job_data)

    async def update_job_status(self, job_id: uuid.UUID, updates: dict[str, Any]) -> RetrainingJob | None:
        """Update job status and results."""
        if "updated_at" not in updates: # Ensure updated_at is set
            updates["updated_at"] = datetime.now(UTC)

        for key in ["start_time", "end_time", "updated_at"]:
            if key in updates and isinstance(updates[key], str):
                 dt_obj = datetime.fromisoformat(updates[key])
                 updates[key] = dt_obj.replace(tzinfo=UTC) if dt_obj.tzinfo is None else dt_obj
            elif key in updates and isinstance(updates[key], datetime) and updates[key].tzinfo is None:
                 updates[key] = updates[key].replace(tzinfo=UTC)

        if "new_model_id" in updates and isinstance(updates["new_model_id"], str):
            updates["new_model_id"] = uuid.UUID(updates["new_model_id"])

        return await self.update(job_id, updates)

    async def get_job(self, job_id: uuid.UUID) -> RetrainingJob | None:
        """Get retraining job by ID."""
        return await self.get_by_id(job_id)

    async def get_recent_jobs(self, days: int = 7) -> Sequence[RetrainingJob]:
        """Get recent retraining jobs."""
        cutoff_date = datetime.now(UTC) - timedelta(days=days)
        async with self.session_maker() as session:
            stmt = (
                select(RetrainingJob)
                .where(RetrainingJob.created_at > cutoff_date)
                .order_by(RetrainingJob.created_at.desc())
            )
            result = await session.execute(stmt)
            return result.scalars().all()

    async def get_jobs_by_model(self, model_id: uuid.UUID) -> Sequence[RetrainingJob]:
        """Get all retraining jobs for a model."""
        return await self.find_all(filters={"model_id": model_id}, order_by="created_at DESC")

    async def save_drift_detection_event(
        self, event_data: dict[str, Any],
    ) -> DriftDetectionEvent:
        """Saves a drift detection event."""
        # event_id is auto-generated by default in DriftDetectionEvent model
        for key in ["model_id"]: # Ensure UUIDs
            if key in event_data and isinstance(event_data[key], str):
                event_data[key] = uuid.UUID(event_data[key])

        if "detected_at" in event_data and isinstance(event_data["detected_at"], str):
            dt_obj = datetime.fromisoformat(event_data["detected_at"])
            event_data["detected_at"] = dt_obj.replace(tzinfo=UTC) if dt_obj.tzinfo is None else dt_obj
        elif "detected_at" in event_data and isinstance(event_data["detected_at"], datetime) and event_data["detected_at"].tzinfo is None:
            event_data["detected_at"] = event_data["detected_at"].replace(tzinfo=UTC)


        if "drift_score" in event_data and not isinstance(event_data["drift_score"], Decimal):
            event_data["drift_score"] = Decimal(str(event_data["drift_score"]))

        async with self.session_maker() as session:
            instance = DriftDetectionEvent(**event_data)
            session.add(instance)
            await session.commit()
            await session.refresh(instance)
            return instance

    async def get_drift_history(
        self, model_id: uuid.UUID, days: int = 30,
    ) -> Sequence[DriftDetectionEvent]:
        """Get drift detection history for a model."""
        cutoff_date = datetime.now(UTC) - timedelta(days=days)
        async with self.session_maker() as session:
            stmt = (
                select(DriftDetectionEvent)
                .where(
                    DriftDetectionEvent.model_id == model_id,
                    DriftDetectionEvent.detected_at > cutoff_date,
                )
                .order_by(DriftDetectionEvent.detected_at.desc())
            )
            result = await session.execute(stmt)
            return result.scalars().all()

    async def get_retraining_metrics(self) -> dict[str, Any]:
        """Get aggregated retraining metrics using raw SQL for complex aggregation."""
        # This query is complex and uses CTEs with specific PostgreSQL functions (row_to_json, json_agg).
        # It's often easier to execute such queries directly with SQLAlchemy's text() construct
        # and then process the results, rather than trying to build it entirely with the ORM/Query builder.

        thirty_days_ago = (datetime.now(UTC) - timedelta(days=30)).isoformat()

        query = text(f"""
            WITH job_stats AS (
                SELECT
                    COUNT(*) as total_jobs,
                    COUNT(CASE WHEN status = 'completed' THEN 1 END) as completed_jobs,
                    COUNT(CASE WHEN status = 'failed' THEN 1 END) as failed_jobs,
                    COUNT(CASE WHEN status = 'running' THEN 1 END) as running_jobs,
                    AVG(EXTRACT(EPOCH FROM (end_time - start_time))) as avg_duration_seconds
                FROM retraining_jobs
                WHERE created_at > '{thirty_days_ago}'
            ),
            trigger_stats AS (
                SELECT
                    trigger,
                    COUNT(*) as count
                FROM retraining_jobs
                WHERE created_at > '{thirty_days_ago}'
                GROUP BY trigger
            ),
            drift_stats AS (
                SELECT
                    drift_type,
                    COUNT(*) as detections,
                    COUNT(CASE WHEN is_significant THEN 1 END) as significant_detections
                FROM drift_detection_events
                WHERE detected_at > '{thirty_days_ago}'
                GROUP BY drift_type
            )
            SELECT
                (SELECT row_to_json(js) FROM job_stats js) as job_statistics,
                (SELECT json_agg(ts) FROM trigger_stats ts) as trigger_distribution,
                (SELECT json_agg(ds) FROM drift_stats ds) as drift_statistics
        """)

        async with self.session_maker() as session:
            result = await session.execute(query)
            row = result.one_or_none() # Expecting a single row with JSON aggregates

            if row:
                # Access columns by name or index. For text() queries with labels, name is usually preferred.
                # If using mappings(), keys will be strings.
                row_mapping = row._mapping # Access the underlying mapping
                return {
                    "job_statistics": row_mapping.get("job_statistics", {}),
                    "trigger_distribution": row_mapping.get("trigger_distribution", []),
                    "drift_statistics": row_mapping.get("drift_statistics", []),
                }
            return { # Default empty structure if no data (e.g., tables are empty)
                "job_statistics": {}, "trigger_distribution": [], "drift_statistics": [],
            }
